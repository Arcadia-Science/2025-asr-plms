{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import ete3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import AlignIO, Phylo, SeqIO\n",
    "from ete3 import Tree\n",
    "from pastml import acr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is modified by papermill for CLI execution during runtime\n",
    "input_fasta = \"placeholder\"\n",
    "outgroup_path = \"placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fasta = Path(input_fasta)\n",
    "outgroup_path = Path(outgroup_path)\n",
    "run_folder = Path(\"ADA1_ASR/outputs\")\n",
    "run_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recode deflines to 10 character max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode them to 10 char max (PAML is limited to phylip format with 10 char max)\n",
    "recoded_input_fasta = run_folder / f\"{input_fasta.stem}_recoded.fa\"\n",
    "recoding_dict_file = run_folder / \"recoding_dict.txt\"\n",
    "\n",
    "counter = 0\n",
    "with open(recoding_dict_file, \"w\") as dict_file:\n",
    "    with open(recoded_input_fasta, \"w\") as seq_file:\n",
    "        for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
    "            seq_name = \"seq\" + str(counter)\n",
    "            seq_file.write(\">\" + seq_name + \"\\n\" + str(record.seq) + \"\\n\")\n",
    "            dict_file.write(seq_name + \"\\t\" + record.id + \"\\n\")\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build alignment with MAFFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafft_out = recoded_input_fasta.with_stem(recoded_input_fasta.stem + \"_mafft\")\n",
    "\n",
    "mafft_cmd = f\"mafft --localpair {recoded_input_fasta} > {mafft_out}\"\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(mafft_cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "    print(f\"Alignment file {mafft_out} generated\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred during execution\")\n",
    "    print(e.stderr)\n",
    "\n",
    "# convert to Phylip sequential format for PAML\n",
    "mafft_phylip = mafft_out.with_suffix(\".ph\")\n",
    "\n",
    "alignment = AlignIO.read(mafft_out, \"fasta\")\n",
    "AlignIO.write(alignment, mafft_phylip, \"phylip-sequential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Model and Build Tree with IQ Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqtree_cmd = f\"iqtree2 -s {mafft_out} -m TESTONLY -nt AUTO -redo\"\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(iqtree_cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out best model from output files and list of models in order\n",
    "\n",
    "all_models = []\n",
    "iqtree_output_path = mafft_out.with_name(f\"{mafft_out.name}.iqtree\")\n",
    "with open(iqtree_output_path) as file:\n",
    "    flag = 0\n",
    "    for line in file:\n",
    "        if \"Best-fit model according to BIC\" in line:\n",
    "            model_name = line.rstrip().split(\":\")[1].replace(\" \", \"\")\n",
    "            print(f\"Best model: {model_name}\")\n",
    "            flag = 1\n",
    "        elif flag == 1:\n",
    "            all_models.append(line.split(\" \")[0])\n",
    "\n",
    "\n",
    "# write best model to separate file to prevent overwriting in treebuilding\n",
    "shutil.move(iqtree_output_path, mafft_out.with_name(f\"{mafft_out.name}.bestmodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tree with best model\n",
    "iqtree_cmd = f\"iqtree2 -s {mafft_out} -m {model_name} -nt AUTO -redo\"\n",
    "print(iqtree_cmd)\n",
    "try:\n",
    "    result = subprocess.run(iqtree_cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "    print(f\"Tree file {mafft_out} generated\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)\n",
    "\n",
    "iqtree_tree = mafft_out.with_name(f\"{mafft_out.name}.treefile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root tree with known outgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root tree with known outgroups\n",
    "outgroup_names = [line.strip() for line in open(outgroup_path).readlines()]\n",
    "\n",
    "# get recoded names of outgroups\n",
    "outgroup_names_recoded = []\n",
    "with open(recoding_dict_file) as file:\n",
    "    for line in file:\n",
    "        names = line.rstrip().split(\"\\t\")\n",
    "        if names[1] in outgroup_names:\n",
    "            outgroup_names_recoded.append(names[0])\n",
    "\n",
    "tree = Phylo.read(iqtree_tree, \"newick\")\n",
    "outgroup_clade = tree.common_ancestor(outgroup_names_recoded[1], outgroup_names_recoded[0])\n",
    "tree.root_with_outgroup(outgroup_clade)\n",
    "\n",
    "rooted_tree = iqtree_tree.with_name(iqtree_tree.stem.split(\".\")[0] + \"_rooted.txt\")\n",
    "Phylo.write(tree, rooted_tree, \"newick\")\n",
    "\n",
    "print(f\"Rooted tree saved to {rooted_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format files for PAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get closest substitution matrix in paml to match best model from iqtree\n",
    "\n",
    "model_dict = {\n",
    "    \"cpREV\": \"cpREV64.dat\",\n",
    "    \"Dayhoff\": \"dayhoff.dat\",\n",
    "    \"DCMut\": \"dayhoff-dcmut.dat\",\n",
    "    \"JTTDCMut\": \"jones-dcmut.dat\",\n",
    "    \"JTT\": \"jones.dat\",\n",
    "    \"LG\": \"lg.dat\",\n",
    "    \"mtART\": \"mtArt.dat\",\n",
    "    \"mtMAM\": \"mtmam.dat\",\n",
    "    \"mtREV\": \"mtREV24.dat\",\n",
    "    \"mtZOA\": \"MtZoa.dat\",\n",
    "    \"WAG\": \"wag.dat\",\n",
    "}\n",
    "\n",
    "for entry in all_models:\n",
    "    if entry.split(\"+\")[0] in model_dict:\n",
    "        paml_model = entry\n",
    "        print(\"best paml-compatible model: \", paml_model)\n",
    "        if \"G4\" in entry:\n",
    "            final_cats = 4\n",
    "        else:\n",
    "            final_cats = 1\n",
    "        print(\"nCatG: \", final_cats)\n",
    "        break\n",
    "\n",
    "final_model = model_dict[paml_model.split(\"+\")[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct ML ancestors with PAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify config file based on sample file\n",
    "sample_config_file = \"codeml_example.ctl\"\n",
    "new_config_file = run_folder / \"codeml.ctl\"\n",
    "\n",
    "conda_env_path = os.environ.get(\"CONDA_PREFIX\")\n",
    "\n",
    "with open(new_config_file, \"w\") as outfile:\n",
    "    with open(sample_config_file) as file:\n",
    "        for line in file:\n",
    "            if \"seqfile\" in line:\n",
    "                outfile.write(f\"seqfile = {mafft_phylip} \\n\")\n",
    "            elif \"treefile\" in line:\n",
    "                outfile.write(f\"treefile = {rooted_tree} \\n\")\n",
    "            elif \"aaRatefile\" in line:\n",
    "                outfile.write(f\"aaRatefile = {conda_env_path + '/dat/' + final_model} \\n\")\n",
    "            elif \"ncatG\" in line:\n",
    "                outfile.write(f\"ncatG = {final_cats} \\n\")\n",
    "\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "print(\n",
    "    f\"config file writted with aaRatefile {os.path.basename(final_model)} and nCatG = {final_cats}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run codeml\n",
    "\n",
    "codeml_cmd = f\"codeml {new_config_file}\"\n",
    "\n",
    "# check if subsitution rate file exists (paml will get stuck otherwise)\n",
    "with open(new_config_file) as file:\n",
    "    for line in file:\n",
    "        if \"aaRatefile\" in line:\n",
    "            rate_file = line.rstrip().split(\"aaRatefile = \")[1]\n",
    "\n",
    "# run paml if rate file exists\n",
    "if not os.path.exists(rate_file):\n",
    "    print(f\"Error: Rate file '{rate_file}' does not exist.\")\n",
    "else:\n",
    "    try:\n",
    "        result = subprocess.run(codeml_cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        if \"error: end of tree file.\" not in e.stderr:  # ignore this error\n",
    "            print(e.stderr)\n",
    "\n",
    "\n",
    "# move output files to run folder\n",
    "outputs = [\"codeml_results.txt\", \"rst\", \"rst1\", \"rub\", \"rates\", \"lnf\"]\n",
    "\n",
    "for entry in outputs:\n",
    "    shutil.move(entry, run_folder / entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse PAML output files to get ML sequences and probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ancestor trees in different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestor_cladogram = run_folder / \"ancestor_cladogram.txt\"\n",
    "ancestor_recoded_cladogram = run_folder / \"ancestor_recoded_cladogram.txt\"\n",
    "ancestor_tree = run_folder / \"ancestor_tree.txt\"\n",
    "ancestor_recoded_tree = run_folder / \"ancestor_recoded_tree.txt\"\n",
    "\n",
    "# pull out ancestor tree string from rst file\n",
    "tree_string = \"\"\n",
    "flag = 0\n",
    "with open(run_folder / \"rst\") as file:\n",
    "    for line in file:\n",
    "        if \"tree with node labels for Rod Page's TreeView\" in line:\n",
    "            flag = 1\n",
    "        elif \"are ancestral\" in line:\n",
    "            flag = 0\n",
    "        elif flag == 1:\n",
    "            tree_string = tree_string + line\n",
    "\n",
    "# remove codeml numbers from sequence names\n",
    "with open(ancestor_recoded_cladogram, \"w\") as file:\n",
    "    file.write(tree_string)\n",
    "\n",
    "terminals = [x.name for x in Phylo.read(ancestor_recoded_cladogram, \"newick\").get_terminals()]\n",
    "for entry in terminals:\n",
    "    tree_string = tree_string.replace(entry + \",\", entry.split(\"_\")[1] + \",\")\n",
    "    tree_string = tree_string.replace(entry + \")\", entry.split(\"_\")[1] + \")\")\n",
    "\n",
    "# remove codeml numbers from sequence names\n",
    "with open(ancestor_recoded_cladogram, \"w\") as file:\n",
    "    file.write(tree_string)\n",
    "\n",
    "# recode ancestor cladogram to original names\n",
    "name_dict = dict(\n",
    "    zip(\n",
    "        [line.rstrip().split(\"\\t\")[0] for line in open(recoding_dict_file)],\n",
    "        [line.rstrip().split(\"\\t\")[1] for line in open(recoding_dict_file)],\n",
    "        strict=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "for entry in name_dict:\n",
    "    tree_string = tree_string.replace(entry + \",\", name_dict[entry] + \",\")\n",
    "    tree_string = tree_string.replace(entry + \")\", name_dict[entry] + \")\")\n",
    "\n",
    "# write to cladogram file\n",
    "with open(ancestor_cladogram, \"w\") as outfile:\n",
    "    outfile.write(tree_string)\n",
    "\n",
    "\n",
    "# Transfer node labels to tree with branch lengths\n",
    "cladogram_tree = Tree(str(ancestor_cladogram), format=1)  # Format 1 preserves internal node names\n",
    "branch_length_tree = Tree(str(rooted_tree), format=1)  # No node names, but correct topology\n",
    "\n",
    "# Extract internal node labels from cladogram\n",
    "cladogram_nodes = cladogram_tree.get_descendants(\"postorder\")\n",
    "branch_nodes = branch_length_tree.get_descendants(\"postorder\")\n",
    "\n",
    "# Create a mapping of cladogram node labels to their topological positions\n",
    "node_label_map = {}\n",
    "for node in cladogram_nodes:\n",
    "    if node.name:  # Internal node labels\n",
    "        node_label_map[id(node)] = node.name\n",
    "\n",
    "# Assign labels to the branch-length tree, assuming same topology\n",
    "for clad_node, branch_node in zip(cladogram_nodes, branch_nodes, strict=False):\n",
    "    if id(clad_node) in node_label_map:\n",
    "        branch_node.name = node_label_map[id(clad_node)]\n",
    "\n",
    "# Save the updated tree with labels\n",
    "branch_length_tree.write(outfile=ancestor_tree, format=1)\n",
    "\n",
    "# REPEAT FOR RECODED TREE\n",
    "# Add node labels to original trees (orig and recoded both)\n",
    "cladogram_tree = Tree(str(ancestor_recoded_cladogram), format=1)  # Format 1 preserves node names\n",
    "branch_length_tree = Tree(str(rooted_tree), format=1)  # No node names, but correct topology\n",
    "\n",
    "# Extract internal node labels from cladogram\n",
    "cladogram_nodes = cladogram_tree.get_descendants(\"postorder\")\n",
    "branch_nodes = branch_length_tree.get_descendants(\"postorder\")\n",
    "\n",
    "# Create a mapping of cladogram node labels to their topological positions\n",
    "node_label_map = {}\n",
    "for node in cladogram_nodes:\n",
    "    if node.name:  # Internal node labels\n",
    "        node_label_map[id(node)] = node.name\n",
    "\n",
    "# Assign labels to the branch-length tree, assuming same topology\n",
    "for clad_node, branch_node in zip(cladogram_nodes, branch_nodes, strict=False):\n",
    "    if id(clad_node) in node_label_map:\n",
    "        branch_node.name = node_label_map[id(clad_node)]\n",
    "\n",
    "# Save the updated tree with labels\n",
    "branch_length_tree.write(outfile=ancestor_recoded_tree, format=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make files with ML Sequences and Posterior Probabilities (all nodes with gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeml_results = run_folder / \"rst\"\n",
    "\n",
    "flag = 0\n",
    "seqs_dict = {}\n",
    "probs_dict = {}\n",
    "res_list = [\n",
    "    \"A\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "]\n",
    "\n",
    "with open(codeml_results) as file:\n",
    "    for line in file:\n",
    "        if \"Prob distribution at node\" in line and \" by site\" in line:\n",
    "            node = \"node\" + line.split(\"node \")[1].split(\",\")[0]\n",
    "            seqs_dict[node] = \"\"\n",
    "            probs_dict[node] = []\n",
    "            flag = 1\n",
    "        elif flag == 1 and \":\" in line:\n",
    "            all_probs = line.rstrip().split(\":\")[1].split(\" \")[1:]\n",
    "            letters = [x.split(\"(\")[0] for x in all_probs]\n",
    "            probs = [float(x.split(\"(\")[1].split(\")\")[0]) for x in all_probs]\n",
    "            support_dict = dict(zip(letters, probs, strict=False))\n",
    "            max_res = max(support_dict, key=support_dict.get)\n",
    "            seqs_dict[node] = seqs_dict[node] + max_res\n",
    "            probs_ordered = [support_dict[x] for x in res_list]\n",
    "            probs_dict[node].append(probs_ordered)\n",
    "\n",
    "        elif \"Prob of best state at each node, listed by site\" in line:\n",
    "            break\n",
    "\n",
    "ml_seqs_with_gaps = run_folder / \"ML_ancestors_with_gaps.fa\"\n",
    "\n",
    "with open(ml_seqs_with_gaps, \"w\") as file:\n",
    "    for entry in seqs_dict:\n",
    "        file.write(\">\" + entry + \"\\n\" + seqs_dict[entry] + \"\\n\")\n",
    "\n",
    "post_probs_file = run_folder / \"posterior_probabilities.json\"\n",
    "\n",
    "with open(post_probs_file, \"w\") as file:\n",
    "    json.dump(probs_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify gap positions based on parsimony using DOWNPASS algorithm from Topiary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_alignment(file_path):\n",
    "    \"\"\"\n",
    "    Read the PHYLIP alignment file and return a gap matrix.\n",
    "    \"\"\"\n",
    "    alignment = AlignIO.read(file_path, \"phylip\")\n",
    "\n",
    "    # Initialize a list to store gap info\n",
    "    taxa_names = [record.id for record in alignment]\n",
    "    num_sites = alignment.get_alignment_length()\n",
    "\n",
    "    # Create a matrix for gaps (True = gap, False = no gap)\n",
    "    gap_matrix = np.zeros((len(taxa_names), num_sites), dtype=np.uint8)\n",
    "\n",
    "    for i, record in enumerate(alignment):\n",
    "        for j, char in enumerate(record.seq):\n",
    "            gap_matrix[i, j] = 1 if char == \"-\" else 0\n",
    "\n",
    "    # Convert the gap matrix into a pandas DataFrame\n",
    "    gap_df = pd.DataFrame(gap_matrix, columns=[f\"g{i}\" for i in range(num_sites)], index=taxa_names)\n",
    "\n",
    "    return gap_df, taxa_names\n",
    "\n",
    "\n",
    "def parse_tree(tree_file):\n",
    "    \"\"\"\n",
    "    Parse the tree and return the tree object.\n",
    "    \"\"\"\n",
    "    tree = ete3.Tree(tree_file, format=1)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def infer_ancestral_gaps(alignment_file, tree_file, prediction_method=\"DOWNPASS\"):\n",
    "    \"\"\"\n",
    "    Infer gaps at ancestral nodes using the acr function from pastml.\n",
    "    \"\"\"\n",
    "    # Step 2: Read the PHYLIP alignment file\n",
    "    gap_df, leaf_names = read_alignment(alignment_file)\n",
    "\n",
    "    # Step 3: Parse the tree file\n",
    "    tree = parse_tree(str(tree_file))\n",
    "\n",
    "    # Step 4: Run the gap reconstruction using the DOWNPASS algorithm\n",
    "    acr.acr(tree, gap_df, prediction_method=prediction_method)\n",
    "\n",
    "    # Step 5: Create a dictionary for gaps in ancestral nodes\n",
    "    gap_anc_dict = {}\n",
    "    for node in tree.traverse(\"preorder\"):\n",
    "        if node.name in leaf_names:\n",
    "            continue  # Skip leaf nodes\n",
    "\n",
    "        gap_anc_dict[node.name] = []\n",
    "\n",
    "        # Collect gap information for each ancestral node\n",
    "        for col in gap_df.columns:\n",
    "            state = node.__dict__.get(col, None)\n",
    "\n",
    "            if isinstance(state, set):\n",
    "                if len(state) == 1:\n",
    "                    state = True if 1 in state else False\n",
    "                else:\n",
    "                    state = None\n",
    "            gap_anc_dict[node.name].append(state)\n",
    "\n",
    "    return gap_anc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_file = mafft_phylip\n",
    "tree_file = ancestor_recoded_tree\n",
    "\n",
    "gap_anc_dict = infer_ancestral_gaps(alignment_file, tree_file, prediction_method=\"DOWNPASS\")\n",
    "\n",
    "# write gaps to json\n",
    "gaps_file = run_folder / \"gap_positions.json\"\n",
    "with open(gaps_file, \"w\") as file:\n",
    "    json.dump(gap_anc_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gap positions to write ungapped ML sequences to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_seqs_no_gaps = run_folder / \"ML_ancestors.fa\"\n",
    "\n",
    "with open(ml_seqs_no_gaps, \"w\") as file:\n",
    "    for record in SeqIO.parse(ml_seqs_with_gaps, \"fasta\"):\n",
    "        node_num = record.id.split(\"node\")[1]\n",
    "        if node_num not in gap_anc_dict:\n",
    "            # no gaps inferred for deepest node (shouldnt reconstruct this anyway)\n",
    "            print(\"no gaps inferred for node \", node_num, \"(root node)\")\n",
    "        else:\n",
    "            full_seq = list(str(record.seq))\n",
    "            trimmed_seq = (\"\").join(\n",
    "                [x for i, x in enumerate(full_seq) if not gap_anc_dict[node_num][i]]\n",
    "            )\n",
    "            file.write(f\">node{node_num}\\n{trimmed_seq}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gap positions to write ungapped probabilities to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_probs_no_gaps = run_folder / \"posterior_probabilities_no_gaps.json\"\n",
    "\n",
    "with open(post_probs_file) as f:\n",
    "    probs_dict = json.load(f)\n",
    "\n",
    "# mask positions with gaps\n",
    "probs_dict_no_gaps = {}\n",
    "\n",
    "for entry in probs_dict:\n",
    "    node_num = entry.split(\"node\")[1]\n",
    "    if node_num not in gap_anc_dict:\n",
    "        print(\"no gaps inferred for node \", node_num, \" (root node)\")\n",
    "    else:\n",
    "        full_matrix_probs = probs_dict[entry]\n",
    "        matrix_masked = [\n",
    "            x for i, x in enumerate(full_matrix_probs) if not gap_anc_dict[node_num][i]\n",
    "        ]\n",
    "        probs_dict_no_gaps[entry] = matrix_masked\n",
    "\n",
    "\n",
    "with open(post_probs_no_gaps, \"w\") as file:\n",
    "    json.dump(probs_dict_no_gaps, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize node support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_support(support_dict, node_num):\n",
    "    full_list = support_dict[node_num]\n",
    "    max_values = [max(x) for x in full_list]\n",
    "\n",
    "    plt.figure(figsize=(5, 3))  # Width x Height in inches\n",
    "    plt.hist(max_values, bins=10, edgecolor=\"black\")\n",
    "\n",
    "    textstr = f\"Mean: {np.mean(max_values):.3f}\"\n",
    "    plt.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        textstr,\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"),\n",
    "    )\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Posterior Probability\")\n",
    "    plt.ylabel(\"# Sites\")\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.title(\"Support for ML Residues\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    # return max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_support(probs_dict_no_gaps, \"node233\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate identity between ancestors and native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_identity(seq1, seq2):\n",
    "    mafft_in = run_folder / \"compare.fa\"\n",
    "    mafft_out = run_folder / \"compare_mafft.fa\"\n",
    "\n",
    "    with open(mafft_in, \"w\") as file:\n",
    "        file.write(\">seq1\" + \"\\n\" + seq1 + \"\\n\")\n",
    "        file.write(\">seq2\" + \"\\n\" + seq2 + \"\\n\")\n",
    "\n",
    "    mafft_cmd = f\"mafft {mafft_in} > {mafft_out}\"\n",
    "\n",
    "    try:\n",
    "        subprocess.run(mafft_cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred during alignment\")\n",
    "        print(e.stderr)\n",
    "\n",
    "    aligned_sequences = [str(record.seq) for record in SeqIO.parse(mafft_out, \"fasta\")]\n",
    "    aligned_pairs = [\n",
    "        (a, b)\n",
    "        for a, b in zip(aligned_sequences[0], aligned_sequences[1], strict=False)\n",
    "        if a != \"-\" and b != \"-\"\n",
    "    ]\n",
    "    matches = sum(a == b for a, b in aligned_pairs)\n",
    "\n",
    "    return (matches / len(aligned_pairs)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestor_of_interest = \"node257\"\n",
    "extant_of_interest = \"P00813_Homo_sapiens\"\n",
    "\n",
    "seq1 = [\n",
    "    str(record.seq)\n",
    "    for record in SeqIO.parse(ml_seqs_no_gaps, \"fasta\")\n",
    "    if record.id == ancestor_of_interest\n",
    "][0]\n",
    "seq2 = [\n",
    "    str(record.seq)\n",
    "    for record in SeqIO.parse(input_fasta, \"fasta\")\n",
    "    if record.id == extant_of_interest\n",
    "][0]\n",
    "\n",
    "print(\n",
    "    f\"{int(calc_identity(seq1, seq2))}% identity between \"\n",
    "    f\"{ancestor_of_interest} and {extant_of_interest}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize tree with ancestral nodes labeled (recommend FigTree for easier viewing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Phylo.read(ancestor_tree, \"newick\")\n",
    "\n",
    "fig = plt.figure(figsize=(15, 60))  # Adjust size (width, height)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "# Customize and render the tree\n",
    "Phylo.draw(\n",
    "    tree,\n",
    "    axes=ax,\n",
    "    label_colors={\"orange\": \"orange\"},  # Specify colors for labels\n",
    "    do_show=False,  # Prevent automatic display\n",
    ")\n",
    "\n",
    "# Manually highlight nodes by drawing them in orange\n",
    "for clade in tree.find_clades():\n",
    "    if hasattr(clade, \"color\") and clade.color == \"orange\":\n",
    "        ax.text(\n",
    "            clade.branch_length,  # X-coordinate\n",
    "            tree.depths()[clade],  # Y-coordinate\n",
    "            clade.name,  # Node label\n",
    "            color=\"orange\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
