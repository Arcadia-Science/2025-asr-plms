{
  "hash": "53c6c7e804a730300031b907540c42f1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"{{< var pub.title >}} --\"\ndate: 'January 1, 2025'\nabstract-title: \"Summary\"\n---\n\n## Abstract\n\nProtein language models are trained on evolutionarily related sequences, yet the extent to which they capture the underlying evolutionary relationships remains unclear. We explore this question using reconstructed ancestral protein sequences and the ESM2 protein language model.\n\n----\n\n:::{.callout-note title=\"AI usage disclosure\" collapse=\"true\"}\nWe used ChatGPT to help write code, clean up code, clarify and streamline text that we wrote, and suggest wording ideas from which we selected specific phrases.\n:::\n\n## Purpose\n\nWe wondered how protein language models trained on extant sequences would interpret plausible ancestral sequences. To explore this, we used ESM2 to evaluate maximum likelihood ancestral sequence reconstructions for two example gene families. We found that ESM2 often finds these ancestral sequences more plausible than extant descendants, and can distinguish between crude consensus ancestral sequences and more sophisticated maximum likelihood reconstructions. However, these patterns are context- and model-dependent, suggesting that further investigation is needed to determine which evolutionary relationships are truly captured by large protein language models like ESM2.\n\nWe are sharing these results to encourage further exploration of how large foundation models interpret the evolutionary relationships embedded in their training data.  We think this could be useful to researchers interested in interrogating the implicit learning in large protein language models, like ESM2, and we propose that ancestral sequences offer a useful tool for this purpose.\n\n## Introduction\n\nProtein language models have emerged as powerful tools for computational biology applications, including fitness prediction, generative protein design, structure prediction, and functional annotation. These models are trained on vast databases of naturally occurring protein sequences with the expectation that they will capture complex relationships underlying these sequences: from fundamental biophysical constraints to evolutionary pressures that have shaped modern proteins (@lin_evolutionary-scale_2023, @hayes_simulating_2025, @bhatnagar_scaling_2025). However, the extent to which these models have learned genuine evolutionary principles versus surface-level sequence patterns, and how this varies across model architectures, remains an open question (@lupo_protein_2022, @ektefaie_sequence_2025, @tule_protein_2025, @york_phylogenies_2025).\n\nAncestral sequence reconstruction (ASR) is a potentially useful framework for addressing this question. ASR employs statistical methods to infer the most probable protein sequences of extinct ancestral organisms based on phylogenetic relationships among extant sequences. While these reconstructions may not represent the exact sequences that existed in the past, they reflect evolutionarily plausible intermediates under established models of molecular evolution, and in many cases have been shown to fold and function (@hochberg_reconstructing_2017). This approach offers two key features for probing evolutionary knowledge encoded in protein language models. First, training datasets consist exclusively of extant sequences. Reconstructed ancestral sequences provide opportunities to evaluate the ability of these models to generalize to new sequences. Second, unlike other types of novel sequences, reconstructed ancestral sequences are unique in their evolutionary plausibility. ASR has been successfully applied to reconstruct and experimentally characterize diverse protein families, demonstrating that many reconstructed sequences represent functionally viable proteins (@bridgham2006, @voordeckers_reconstruction_2012, @wilson_using_2015). Thus, ancestral proteins provide an opportunity to assess model confidence in evolutionarily plausible sequences never observed during training and infer the generalizability of evolutionary knowledge contained within pLMs.  \n\nHere, we focus on the ESM2 protein language model (@lin_evolutionary-scale_2023), one of the most widely used and well-characterized models to date. ESM2 is trained on millions of extant protein sequences from UniRef and has been applied to a wide range of prediction tasks. It includes multiple model sizes, ranging from 8 million to 15 billion parameters, allowing us to examine how model behavior varies with scale. Using reconstructed ancestral protein sequences from two protein families, we find that ESM2 often assigns higher plausibility to ancestral sequences than to their extant descendants and can distinguish between crude consensus and maximum likelihood reconstructions, particularly for larger ESM2 model sizes. However, we find these relationships to vary significantly across ESM2 model sizes, suggesting the evolutionary learning is inconsistent. This supports the idea that large language models like ESM2 encode evolutionary signal, but indicates that further investigation is needed to determine the extent of this learning.\n\n## ADA1 ancestral reconstruction\n\nWe began this analysis with the [human protein ADA1](https://www.uniprot.org/uniprotkb/P00813/entry), an adenosine deaminase that plays a crucial role in purine metabolism. ADA1 offered several advantages as a test case: it is relatively small (363 amino acids), making computationally intensive pseudo-perplexity calculations more tractable, and is relatively well conserved across taxa, allowing for high-confidence sequence alignments and ancestral reconstructions.\n\nWe performed ASR on ADA1 by identifying homologs using Protein Cartography (v0.0.2) (@avasthi_proteincartography_2023), aligning proteins with MAFFT (@katoh_mafft_2002), building a phylogenetic tree with IQTree (@nguyen_iq-tree_2015), reconstructing maximum likelihood ancestral sequences using PAML (@yang_paml_2007), and inferring insertions and deletions using PastML (@ishikawa_fast_2019) (as in @orlandi_topiary_2023).\n\n::: {.callout-note}\nWe performed ancestral reconstructions using a separate notebook, [ASR/ASR_notebook.ipynb](https://github.com/Arcadia-Science/2025-asr-plms/blob/main/ASR/ASR_notebook.ipynb). This step was conducted separately to keep the main analysis here focused and uncluttered, as ancestral reconstruction involves a multi-stage pipeline with its own set of dependencies and intermediate steps. [ASR/README.md](https://github.com/Arcadia-Science/2025-asr-plms/blob/main/ASR/README.ipynb) provides instructions for reproducing the ancestral sequences used in the present analysis.\n:::\n\nTo determine the confidence of the ESM2 model in these ancestral sequences, we used pseudo-perplexity as our primary metric (@lin_evolutionary-scale_2023 , @salazar_masked_2020). Pseudo-perplexity quantifies how well a protein language model can predict each amino acid in a sequence given the surrounding context, essentially measuring the model’s \"surprise\" at encountering each individual residue. A pseudo-perplexity value near 20 indicates that the model is highly uncertain and assigns roughly equal probability to all 20 amino acids, while a value closer to 1 indicates strong confidence in its prediction. In practice, pseudo-perplexity is computed by summing the negative log-likelihoods of each residue prediction across the entire sequence, yielding a sequence-level score that reflects how grammatically consistent a model believes the sequence to be based on its training on natural proteins. Lower pseudo-perplexity values, therefore, suggest higher plausibility under the model’s learned distribution. By analyzing pseudo-perplexity scores across ancestral sequences with different evolutionary age, we aimed to assess whether protein language models can infer the plausibility of ancestrally reconstructed sequences and how their confidence varies across evolutionary time.\n\n## ADA1 phylogeny\n\nWe first wanted to assess how ESM2 pseudo-perplexity varies between ancestral and native sequences and the effect of ancestral age. To start, we looked at the human ADA1 protein P00813.\n\n::: {#cell-fig-1 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport arcadia_pycolor as apc\n\nfrom src.analysis.data_processing import plot_image_with_arrow_and_circles, plot_image_with_arrows\n\napc.mpl.setup()\n\nimg_path = \"images/ADA1_tree.png\"\nplot_image_with_arrows(\n    img_path,\n    x_starts=[0.73],\n    x_ends=[0.60],\n    arrow_ys=[0.045],\n    labels=[\"P00813_Homo_sapiens\"],\n    text_offsets=[0.01],\n)\n```\n\n::: {.cell-output .cell-output-display}\n![Phylogenetic tree of ADA1 homologs with location of human ADA1 indicated by arrow.](index_files/figure-html/fig-1-output-1.png){#fig-1}\n:::\n:::\n\n\nWe determined every ancestral node leading to this extant leaf and its maximum likelihood ancestral sequence, as shown in the example below.\n\n::: {#cell-fig-2 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nnode_img_path = \"images/tree_nodes_example.png\"\nplot_image_with_arrow_and_circles(\n    img_path=node_img_path,\n    x_start=0.97,\n    x_end=0.89,\n    arrow_y=0.01,\n    circle_positions=[\n        (0.771, 0.025),\n        (0.70, 0.058),\n        (0.675, 0.105),\n        (0.604, 0.155),\n        (0.553, 0.197),\n        (0.48, 0.23),\n        (0.335, 0.27),\n        (0.285, 0.335),\n        (0.178, 0.41),\n        (0.146, 0.515),\n        (0.066, 0.625),\n    ],\n    circle_radius=6,\n)\n```\n\n::: {.cell-output .cell-output-display}\n![Example of all ancestral nodes (orange circles) on the lineage of an example leaf of interest (orange arrow). The basal node of the tree was not included, as our ASR pipeline does not reconstruct this node.](index_files/figure-html/fig-2-output-1.png){#fig-2}\n:::\n:::\n\n\n## Loading the data\n\nWe calculated pseudo-perplexity scores using the 650M ESM2 model for relevant ancestral sequences and extant sequences in our dataset to begin inferring model confidence of reconstructed sequences.\n\n::: {.callout-note}\nWe performed pseudo-perplexity calculations using the script [`ESM2_scoring/esm2_pppl_calculator.py`](https://github.com/Arcadia-Science/2025-asr-plms/blob/main/ESM2_scoring/esm2_pppl_calculator.py). This step was conducted separately because it required GPU compute for efficient execution. [ESM2_scoring/README.md](https://github.com/Arcadia-Science/2025-asr-plms/blob/main/ESM2_scoring/esm2_pppl_calculator.py) provides instructions for reproducing the perplexity scores used in the present analysis.\n:::\n\nIn addition to pseudo-perplexity values, we extracted the mean posterior probabilities for each ancestral reconstruction from the PAML output. Posterior probability in this context represents the statistical confidence of the maximum likelihood reconstruction.  Specifically, it quantifies the probability that each amino acid at each position in the ancestral sequence is correctly inferred given the appropriate phylogenetic model and observed extant sequences. Higher values, close to 1.0, indicate high confidence in the protein reconstruction. \n\nTo quantify evolutionary divergence, we calculated the total branch length from the root to each ancestral node and extant leaf in our phylogenetic tree. This allowed us to examine how ESM2 confidence varies across evolutionary age.\n\n::: {#1af4d91c .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom Bio import Phylo\n\nesm2_dir = Path(\"ESM2_scoring/outputs/\")\n\n# Import ADA1 sequences scored for ESM2 pppl\nall_scores = pd.read_csv(esm2_dir / \"ADA1_all_esm2_scores_650M.csv\")\nall_scores = all_scores[[\"sequence_id\", \"sequence\", \"pseudo_perplexity\"]]\n\n# Retrieve ML posterior probabilities from ASR run\nml_probs_json = Path(\"ASR/ADA1_ASR/outputs/posterior_probabilities_no_gaps.json\")\nwith open(ml_probs_json) as f:\n    probs_dict = json.load(f)\n\n# Add posterior probabilities to dataframe\nall_scores[\"ML prob\"] = all_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict[x]]) if x in probs_dict else np.nan\n)\n\n# Import phylogenetic tree to infer ancestors of leaf of interest\ntree_file = \"ASR/ADA1_ASR/outputs/ancestor_tree.txt\"\ntree = Phylo.read(tree_file, \"newick\")\n\n# Calculate the branch length to the root for every ancestor and native sequence\nfrom src.analysis.data_processing import (\n    calc_branch_length_to_root_leaf,\n    calc_branch_length_to_root_node,\n)\n\nall_scores[\"bl_to_root\"] = all_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(tree, x)\n)\n```\n:::\n\n\n## Visualizing ESM2 pseudo-perplexity vs. evolutionary distance\n\nPlotting ESM2 pseudo-perplexity scores against evolutionary distance from the tree root allows us to examine the effect of both ancestral age (distance to tree root) and reconstruction confidence (posterior probability) on the pseudo-perplexity score.\n\n::: {#cell-fig-3 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nfrom src.analysis.data_processing import get_node_labels_leaf_to_root, plot_evo_path\n\nplot_evo_path(all_scores, tree, \"P00813_Homo_sapiens\")\n```\n\n::: {.cell-output .cell-output-display}\n![ESM2 pseudo-perplexity scores for ADA1 sequences, colored by ML posterior probability bin. The x-axis shows the branch length to the root node of the tree, with the extant sequence on the far right (orange). The y-axis shows ESM2 pseudo-perplexity score.](index_files/figure-html/fig-3-output-1.png){#fig-3}\n:::\n:::\n\n\nESM2 assigns lower pseudo-perplexity scores to recent ancestral sequences compared to the extant human sequence, suggesting that the model considers these reconstructed ancestors more plausible than the native sequence. This is intriguing given that these ancestral sequences were never present in the training data, and may indicate that ESM2 has inferred specific evolutionary patterns sufficiently to recognize some high-confidence ancestral reconstructions as sound.\n\nHowever, as evolutionary distance increases, pseudo-perplexity scores rise and eventually surpass the native score. This suggests the model finds these basal ancestors less plausible than the native and more recent ancestral sequences. \n\nWe wondered how general this phenomenon was, so we examined three additional extant leaves in our ADA1 phylogenetic tree and ancestral nodes on their respective lineages.\n\n::: {#cell-fig-4 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nplot_image_with_arrows(\n    img_path,\n    x_starts=[0.73, 0.89, 0.86, 0.78],\n    x_ends=[0.60, 0.76, 0.73, 0.65],\n    arrow_ys=[0.045, 0.44, 0.305, 0.745],\n    labels=[\n        \"P00813_Homo_sapiens\",\n        \"A0A8S4BYQ7_Menidia_menidia\",\n        \"A0A673JME8_Sinocyclocheilus_rhinocerous\",\n        \"A0A8J6K0P8_Eleutherodactylus_coqui\",\n    ],\n    text_offsets=[0.01, 0.01, 0.01, 0.01],\n)\n```\n\n::: {.cell-output .cell-output-display}\n![Phylogenetic tree of ADA1 with additional extant leaf positions indicated by arrows.](index_files/figure-html/fig-4-output-1.png){#fig-4}\n:::\n:::\n\n\n::: {#fig-5 .cell .column-body execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nplot_evo_path(all_scores, tree, \"A0A673JME8_Sinocyclocheilus_rhinocerous\")\nplot_evo_path(all_scores, tree, \"A0A8S4BYQ7_Menidia_menidia\")\nplot_evo_path(all_scores, tree, \"A0A8J6K0P8_Eleutherodactylus_coqui\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-5-output-1.png){#fig-5-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-5-output-2.png){#fig-5-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-5-output-3.png){#fig-5-3}\n:::\n\nESM2 pseudo-perplexity vs distance to tree root for three additional species lineages.\n:::\n\n\nHere we see a similar pattern: recent ancestors show lower pseudo-perplexity scores than their native descendants, but these scores increase for the oldest ancestors. Unlike human ADA1, however, none of these other ancestors showed higher pseudo-perplexity scores than their native sequences, suggesting this isn't universal. This may reflect the lower baseline score of human ADA1 compared to the others, perhaps due to over-representation of human and closely related sequences in the training set compared to these other species. However, we consistently observe that ESM2 considers many ancestral sequences more plausible than native sequences across different species.\n\n## Assessing the role of consensus effects\n\nWe wondered whether ESM’s general preference for ancestral sequences reflects primarily a consensus effect in these models. Maximum likelihood ancestral reconstructions, while statistically based, often favor amino acids that are common among descendant sequences, potentially creating sequences that align with the frequency distributions in the ESM2 training data. Therefore, ESM2 might prefer these sequences simply because they contain high-frequency amino acids.\n\nTo test this hypothesis, we generated \"consensus ancestors\" as controls. For each ancestral node, we identified all extant sequences descending from that node. We constructed a consensus sequence by choosing the most frequent amino acid found at each position in the alignment. This relatively simple approach differs fundamentally from maximum likelihood reconstruction, which incorporates evolutionary models, branch lengths, and substitution matrices to infer the most probable ancestral state given the entire phylogenetic context.\n\n::: {#54b05f93 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nfrom Bio.Seq import Seq\nfrom Bio.SeqRecord import SeqRecord\n\nfrom src.analysis.data_processing import generate_node_consensus, get_node_labels_leaf_to_root\n\nleaves_of_interest = [\n    \"P00813_Homo_sapiens\",\n    \"A0A8S4BYQ7_Menidia_menidia\",\n    \"A0A673JME8_Sinocyclocheilus_rhinocerous\",\n    \"A0A8J6K0P8_Eleutherodactylus_coqui\",\n]\nancestors_of_interest = []\n\nfor entry in leaves_of_interest:\n    ancestors_of_interest.extend(get_node_labels_leaf_to_root(tree, entry))\n\nancestors_of_interest = list(set(ancestors_of_interest))\n\nalignment_file = \"ASR/ADA1_ASR/outputs/ADA1_curated_022525_under420_recoded_mafft.fa\"\nname_conv_dict = \"ASR/ADA1_ASR/outputs/recoding_dict.txt\"\ngap_dict_json = \"ASR/ADA1_ASR/outputs/gap_positions.json\"\n\nconsensus_seq_file = esm2_dir.parent / \"inputs\" / \"consensus_ADA1_ancestors.fa\"\n\nrecords = []\nfor entry in ancestors_of_interest:\n    consensus_seq = generate_node_consensus(\n        tree, entry, alignment_file, name_conv_dict, gap_dict_json\n    )\n    name = entry + \"_consensus\"\n    record = SeqRecord(Seq(consensus_seq), id=name, description=\"\")\n    records.append(record)\n```\n:::\n\n\nWe then calculated pseudo-perplexity scores for these consensus ancestors using the ESM2 650M-parameter model.\n\n::: {#af8ec43d .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# retrieve esm2 scores for consensus seqs\nconsensus_file = esm2_dir / \"consensus_ADA1_ancestors_esm2_scores_650M.csv\"\nconsensus_scores = pd.read_csv(consensus_file)\nconsensus_scores[\"orig_id\"] = consensus_scores[\"sequence_id\"].apply(\n    lambda x: x.replace(\"_consensus\", \"\")\n)\nconsensus_scores = consensus_scores[[\"orig_id\", \"sequence\", \"pseudo_perplexity\"]]\nconsensus_scores = consensus_scores.rename(\n    columns={\n        \"sequence\": \"consensus_seq\",\n        \"pseudo_perplexity\": \"consensus_pppl\",\n        \"orig_id\": \"sequence_id\",\n    }\n)\n\n\n# add new consensus seqs and scores to scores df\nall_scores = all_scores.merge(consensus_scores, how=\"left\", on=\"sequence_id\")\n```\n:::\n\n\nBelow, we compared the ESM2 pseudo-perplexity scores of these consensus ancestors to the scores of genuine maximum likelihood ancestors for the four extant sequences we previously examined. \n\n::: {#fig-6 .cell .column-body execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nfrom src.analysis.data_processing import plot_evo_path_quiver\n\nplot_evo_path_quiver(all_scores, tree, \"P00813_Homo_sapiens\", \"upper right\")\nplot_evo_path_quiver(all_scores, tree, \"A0A8S4BYQ7_Menidia_menidia\")\nplot_evo_path_quiver(all_scores, tree, \"A0A673JME8_Sinocyclocheilus_rhinocerous\")\nplot_evo_path_quiver(all_scores, tree, \"A0A8J6K0P8_Eleutherodactylus_coqui\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-6-output-1.png){#fig-6-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-6-output-2.png){#fig-6-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-6-output-3.png){#fig-6-3}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-6-output-4.png){#fig-6-4}\n:::\n\nESM2 pseudo-perplexity (pppl) scores for consensus and maximum likelihood (ML) ancestors of ADA1, with arrows indicating which ancestor has the lower pseudo-perplexity score.\n:::\n\n\nMaximum likelihood ancestors frequently show lower pseudo-perplexity scores than their corresponding consensus ancestors. However, both types of ancestral sequences often scored lower than extant sequences, indicating that a consensus effect partially drives the observed preference for ancestral sequences.\n\nHowever, the relationship between maximum likelihood and consensus ancestors varies with evolutionary distance. For recent ancestors, maximum likelihood sequences consistently outperform consensus ancestors, suggesting that ESM2 has learned to recognize evolutionarily informed reconstructions as more plausible than simple frequency-based sequences. This indicates that the model has internalized aspects of evolutionary relationships beyond mere amino acid frequency distributions. However, this relationship reverses for more ancient ancestors, where consensus sequences often achieve lower pseudo-perplexity scores than maximum likelihood reconstructions. These ancient reconstructions also exhibit lower posterior probabilities, so this reversal may reflect increased uncertainty and potential errors in the maximum likelihood reconstructions. Alternatively, this behavior could reflect over-fitting to native sequences, given that consensus sequences likely more closely match the frequency distributions of the training set. While it’s difficult to distinguish between these two factors for the oldest ancestors, for more recent ancestors it’s clear that ESM2 can infer that maximum likelihood ancestors are significantly more plausible than crude consensus ancestors.\n\n\n## Comparing different ESM2 model sizes\n\nWe were curious to know how much of the evolutionary information ESM2 seems to be learning is consistent across different model sizes or whether this information is only captured in large models. To investigate this, we repeated the above analysis with different ESM2 model sizes: 8M, 35M, 150M, and 3B parameters (in addition to the previously shown 650M). \n\n::: {#ce7366a4 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nsmall_model_scores = pd.read_csv(esm2_dir / \"ADA1_all_esm2_scores_8M.csv\")\nmed_model_scores = pd.read_csv(esm2_dir / \"ADA1_all_esm2_scores_35M.csv\")\nlarge_model_scores = pd.read_csv(esm2_dir / \"ADA1_all_esm2_scores_150M.csv\")\nhuge_model_scores = pd.read_csv(esm2_dir / \"ADA1_all_esm2_scores_3B.csv\")\n\nsmall_model_scores[\"ML prob\"] = small_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict[x]]) if x in probs_dict else np.nan\n)\nsmall_model_scores[\"bl_to_root\"] = small_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(tree, x)\n)\n\nmed_model_scores[\"ML prob\"] = med_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict[x]]) if x in probs_dict else np.nan\n)\nmed_model_scores[\"bl_to_root\"] = med_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(tree, x)\n)\n\nlarge_model_scores[\"ML prob\"] = large_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict[x]]) if x in probs_dict else np.nan\n)\nlarge_model_scores[\"bl_to_root\"] = large_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(tree, x)\n)\n\nhuge_model_scores[\"ML prob\"] = huge_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict[x]]) if x in probs_dict else np.nan\n)\nhuge_model_scores[\"bl_to_root\"] = huge_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(tree, x)\n)\n```\n:::\n\n\nWe can then plot the same relationship between ESM2 pseudo-perplexity and distance to tree root for these different model sizes to see if they exhibit similar behavior. \n\n::: {#fig-7 .cell .column-body execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nfrom src.analysis.data_processing import plot_multiple_evo_lines\n\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"P00813_Homo_sapiens\", \"8M\"),\n        (med_model_scores, \"P00813_Homo_sapiens\", \"35M\"),\n        (large_model_scores, \"P00813_Homo_sapiens\", \"150M\"),\n        (all_scores, \"P00813_Homo_sapiens\", \"650M\"),\n        (huge_model_scores, \"P00813_Homo_sapiens\", \"3B\"),\n    ],\n    tree,\n    normalize=False,\n)\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"8M\"),\n        (med_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"35M\"),\n        (large_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"150M\"),\n        (all_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"650M\"),\n        (huge_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"3B\"),\n    ],\n    tree,\n    normalize=False,\n)\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"8M\"),\n        (med_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"35M\"),\n        (large_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"150M\"),\n        (all_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"650M\"),\n        (huge_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"3B\"),\n    ],\n    tree,\n    normalize=False,\n)\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"8M\"),\n        (med_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"35M\"),\n        (large_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"150M\"),\n        (all_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"650M\"),\n        (huge_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"3B\"),\n    ],\n    tree,\n    normalize=False,\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-7-output-1.png){#fig-7-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-7-output-2.png){#fig-7-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-7-output-3.png){#fig-7-3}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-7-output-4.png){#fig-7-4}\n:::\n\nESM2 pseudo-perplexity vs distance to tree root for five different ESM2 model sizes for four species lineages.\n:::\n\n\nLarger models consistently produce overall lower pseudo-perplexity scores, reflecting their improved ability to learn protein sequence patterns. However, the relationship between pseudo-perplexity and evolutionary age varies across model sizes.  To facilitate easier comparison across models, we normalized all pseudo-perplexity values to each model's extant sequence pseudo-perplexity score.\n\n::: {#fig-8 .cell .column-body execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"P00813_Homo_sapiens\", \"8M\"),\n        (med_model_scores, \"P00813_Homo_sapiens\", \"35M\"),\n        (large_model_scores, \"P00813_Homo_sapiens\", \"150M\"),\n        (all_scores, \"P00813_Homo_sapiens\", \"650M\"),\n        (huge_model_scores, \"P00813_Homo_sapiens\", \"3B\"),\n    ],\n    tree,\n)\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"8M\"),\n        (med_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"35M\"),\n        (large_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"150M\"),\n        (all_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"650M\"),\n        (huge_model_scores, \"A0A8S4BYQ7_Menidia_menidia\", \"3B\"),\n    ],\n    tree,\n)\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"8M\"),\n        (med_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"35M\"),\n        (large_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"150M\"),\n        (all_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"650M\"),\n        (huge_model_scores, \"A0A673JME8_Sinocyclocheilus_rhinocerous\", \"3B\"),\n    ],\n    tree,\n)\nplot_multiple_evo_lines(\n    [\n        (small_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"8M\"),\n        (med_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"35M\"),\n        (large_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"150M\"),\n        (all_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"650M\"),\n        (huge_model_scores, \"A0A8J6K0P8_Eleutherodactylus_coqui\", \"3B\"),\n    ],\n    tree,\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-8-output-1.png){#fig-8-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-8-output-2.png){#fig-8-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-8-output-3.png){#fig-8-3}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-8-output-4.png){#fig-8-4}\n:::\n\nNormalized ESM2 pseudo-perplexity vs distance to tree root for different ESM2 model sizes. For each species lineage, the pseudo-perplexity scores for each model are normalized to the score for the extant sequence for that model size.\n:::\n\n\nThis normalization reveals distinct differences in how models of varying sizes score ancestors of different ages. The larger models (650M and 3B parameters) show dramatic increases in pseudo-perplexity for the oldest ancestors compared to recent ancestors or extant sequences, while the smallest models show only gradual increases for these ancient sequences. Conversely, the largest models demonstrate the most significant decreases in pseudo-perplexity for the most recent ancestors relative to extant sequences.\n\nThese patterns suggest the models learn evolutionary principles to markedly different extents. The largest models disfavor low-confidence reconstructions while favoring high-confidence reconstructions. This could reflect genuine evolutionary learning or overfitting to native-like sequences. Meanwhile, the smallest models show minimal pseudo-perplexity differences from native to deepest ancestors, though their absolute scores remain relatively high (> 10) compared to the much lower scores (< 3) of larger models.\n\nMost importantly, these results demonstrate that ESM2 model size is critically essential for evolutionary analysis. Depending on which model size you use for this analysis, you could reach vastly different conclusions: that all ancestors are more plausible than native sequences, that some ancestors are more plausible, that ancestors and native sequences are similarly plausible, or that ancestral age strongly affects plausibility.\n\n## Second example gene: Yeast isomaltase\n\nWe examined [yeast isomaltase](https://www.uniprot.org/uniprotkb/A0A420MWB1/entry) to explore whether these findings generalized to other proteins. Isomaltase produced high-quality alignments and phylogenetic trees, was relatively small for pseudo-perplexity calculations (589 amino acids), and showed a phylogenetic structure similar to ADA1. Ancestral reconstructions displayed high confidence toward terminal leaves and lower (but still reasonable) confidence at basal nodes. While not all gene phylogenies follow this pattern, we wanted to understand how ESM2 behaves with this fairly typical phylogenetic structure. Future work could expand this analysis to genes with more diverse phylogenetic patterns to examine how pseudo-perplexity varies across different tree topologies.\n\nWe selected three extant leaves to interrogate in the same manner as above, their location on the isomaltase phylogenetic tree is shown below.\n\n::: {#cell-fig-9 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nimg_path = \"images/isomaltase_tree.png\"\nplot_image_with_arrows(\n    img_path,\n    x_starts=[0.94, 0.96, 0.88],\n    x_ends=[0.81, 0.83, 0.75],\n    arrow_ys=[0.105, 0.343, 0.56],\n    labels=[\n        \"A0A420MWB1_Fusarium_oxysporum\",\n        \"A0A177DQL5_Alternaria_alternata\",\n        \"A0A4T0C5S8_Aureobasidium_pullulans\",\n    ],\n    text_offsets=[0.01, 0.01, 0.01],\n)\n```\n\n::: {.cell-output .cell-output-display}\n![Phylogenetic tree of isomaltase with extant leaf positions indicated by arrows.](index_files/figure-html/fig-9-output-1.png){#fig-9}\n:::\n:::\n\n\nIt's worth noting that the scale of substitutions per site is much smaller for isomaltase than ADA1, indicating that these sequences are less divergent (maximum branch length ~1.5 vs. ~2.5 for ADA1). Correspondingly, the overall ASR mean posterior probabilities are also higher.\n\nWe can then plot the ESM2 pseudo-perplexity (650M-parameter model) compared to the distance to tree root as we did for ADA1 for all three leaves.\n\n::: {#fig-10 .cell .column-body execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\n# get ESM2 scores for isomaltase\niso_scores = pd.read_csv(esm2_dir / \"isomaltase_all_esm2_scores_650M.csv\")\n\n# retrieve the dictionary of probabilities from PAML output\nml_probs_json = \"ASR/Isomaltase_ASR/outputs/posterior_probabilities_no_gaps.json\"\nml_probs_json = Path(ml_probs_json)\nwith open(ml_probs_json) as f:\n    iso_probs_dict = json.load(f)\n\niso_scores[\"ML prob\"] = iso_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in iso_probs_dict[x]]) if x in iso_probs_dict else np.nan\n)\n\ntree_file = \"ASR/Isomaltase_ASR/outputs/ancestor_tree.txt\"\niso_tree = Phylo.read(tree_file, \"newick\")\n\niso_scores[\"bl_to_root\"] = iso_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(iso_tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(iso_tree, x)\n)\n\nplot_evo_path(iso_scores, iso_tree, \"A0A420MWB1_Fusarium_oxysporum\", labels=False)\nplot_evo_path(iso_scores, iso_tree, \"A0A177DQL5_Alternaria_alternata\", labels=False)\nplot_evo_path(iso_scores, iso_tree, \"A0A4T0C5S8_Aureobasidium_pullulans\", labels=False)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-10-output-1.png){#fig-10-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-10-output-2.png){#fig-10-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-10-output-3.png){#fig-10-3}\n:::\n\nESM2 pseudo-perplexity scores for isomaltase sequences, colored by ML posterior probability bin. The x-axis shows the branch length to the root node of the tree, with the extant sequence on the far right (orange). The y-axis shows ESM2 pseudo-perplexity score.\n:::\n\n\nIsomaltase reveals similar but attenuated patterns compared to ADA1: pseudo-perplexity decreases among recent ancestors relative to extant sequences, though less pronounced than ADA1. This is followed by leveling off and slight increases for the oldest ancestors, again much less dramatic than the ADA1 pattern. These results confirm that ESM2's preference for ancestral sequences over native sequences extends to this second gene family. However, we don't observe the sharp increase in pseudo-perplexity seen with ADA1's most ancient ancestors, likely because isomaltase lacks reconstructions with comparably low posterior probabilities or similar branch length divergence.\n\nWe also generated consensus ancestral sequences for isomaltase and compared their ESM2 pseudo-perplexity scores to maximum likelihood ancestral sequences.\n\n::: {#fig-11 .cell .column-body execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\n# generate isomaltase consensus ancestors\n\nleaves_of_interest = [\n    \"A0A420MWB1_Fusarium_oxysporum\",\n    \"A0A177DQL5_Alternaria_alternata\",\n    \"A0A4T0C5S8_Aureobasidium_pullulans\",\n]\nancestors_of_interest = []\n\nfor entry in leaves_of_interest:\n    ancestors_of_interest.extend(get_node_labels_leaf_to_root(iso_tree, entry))\n\nancestors_of_interest = list(set(ancestors_of_interest))\n\nalignment_file = \"ASR/Isomaltase_ASR/outputs/isomaltase_dereplicated_final_recoded_mafft.fa\"\nname_conv_dict = \"ASR/Isomaltase_ASR/outputs/recoding_dict.txt\"\ngap_dict_json = \"ASR/Isomaltase_ASR/outputs/gap_positions.json\"\n\nconsensus_seq_file = \"consensus_isomaltase_ancestors.fa\"\n\nrecords = []\nfor entry in ancestors_of_interest:\n    consensus_seq = generate_node_consensus(\n        iso_tree, entry, alignment_file, name_conv_dict, gap_dict_json\n    )\n    name = entry + \"_consensus\"\n    record = SeqRecord(Seq(consensus_seq), id=name, description=\"\")\n    records.append(record)\n\n# retrieve esm2 scores for consensus seqs\nconsensus_file = esm2_dir / \"consensus_isomaltase_ancestors_esm2_scores_650M.csv\"\nconsensus_scores = pd.read_csv(consensus_file)\nconsensus_scores[\"orig_id\"] = consensus_scores[\"sequence_id\"].apply(\n    lambda x: x.replace(\"_consensus\", \"\")\n)\nconsensus_scores = consensus_scores[[\"orig_id\", \"sequence\", \"pseudo_perplexity\"]]\nconsensus_scores = consensus_scores.rename(\n    columns={\n        \"sequence\": \"consensus_seq\",\n        \"pseudo_perplexity\": \"consensus_pppl\",\n        \"orig_id\": \"sequence_id\",\n    }\n)\n\n# add new consensus seqs and scores to scores df\niso_scores = iso_scores.merge(consensus_scores, how=\"left\", on=\"sequence_id\")\n\nplot_evo_path_quiver(iso_scores, iso_tree, \"A0A420MWB1_Fusarium_oxysporum\", \"lower right\")\nplot_evo_path_quiver(iso_scores, iso_tree, \"A0A177DQL5_Alternaria_alternata\")\nplot_evo_path_quiver(iso_scores, iso_tree, \"A0A4T0C5S8_Aureobasidium_pullulans\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-11-output-1.png){#fig-11-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-11-output-2.png){#fig-11-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-11-output-3.png){#fig-11-3}\n:::\n\nESM2 pseudo-perplexity (pppl) scores for consensus and maximum likelihood (ML) ancestors of ADA1, with arrows indicating which ancestor has the lower pseudo-perplexity score.\n:::\n\n\nThis comparison shows distinct behavior from ADA1. In virtually all cases, pseudo-perplexity scores for maximum likelihood ancestors were lower than those for consensus ancestors. Overall, the model greatly prefers maximum likelihood ancestors for isomaltase, suggesting that the phylogenetic signal has been largely learned by the model rather than simple amino acid frequency distributions.\n\nWe also tested these sequences using different-sized ESM2 models to see if we observe differences between model sizes, as with ADA1. Below are the normalized scores.\n\n::: {#fig-12 .cell .column-body execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\n# load esm2 scores for isomaltase\n\niso_small_model_scores = pd.read_csv(esm2_dir / \"isomaltase_all_esm2_scores_8M.csv\")\niso_med_model_scores = pd.read_csv(esm2_dir / \"isomaltase_all_esm2_scores_35M.csv\")\niso_large_model_scores = pd.read_csv(esm2_dir / \"isomaltase_all_esm2_scores_150M.csv\")\niso_huge_model_scores = pd.read_csv(esm2_dir / \"isomaltase_all_esm2_scores_3B.csv\")\n\niso_small_model_scores[\"ML prob\"] = iso_small_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in iso_probs_dict[x]]) if x in iso_probs_dict else np.nan\n)\niso_small_model_scores[\"bl_to_root\"] = iso_small_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(iso_tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(iso_tree, x)\n)\n\niso_med_model_scores[\"ML prob\"] = iso_med_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in iso_probs_dict[x]]) if x in iso_probs_dict else np.nan\n)\niso_med_model_scores[\"bl_to_root\"] = iso_med_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(iso_tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(iso_tree, x)\n)\n\niso_large_model_scores[\"ML prob\"] = iso_large_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in iso_probs_dict[x]]) if x in iso_probs_dict else np.nan\n)\niso_large_model_scores[\"bl_to_root\"] = iso_large_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(iso_tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(iso_tree, x)\n)\n\niso_huge_model_scores[\"ML prob\"] = iso_huge_model_scores[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in iso_probs_dict[x]]) if x in iso_probs_dict else np.nan\n)\niso_huge_model_scores[\"bl_to_root\"] = iso_huge_model_scores[\"sequence_id\"].apply(\n    lambda x: calc_branch_length_to_root_node(iso_tree, x[4:])\n    if x.startswith(\"node\")\n    else calc_branch_length_to_root_leaf(iso_tree, x)\n)\n\nplot_multiple_evo_lines(\n    [\n        (iso_small_model_scores, \"A0A420MWB1_Fusarium_oxysporum\", \"8M\"),\n        (iso_med_model_scores, \"A0A420MWB1_Fusarium_oxysporum\", \"35M\"),\n        (iso_large_model_scores, \"A0A420MWB1_Fusarium_oxysporum\", \"150M\"),\n        (iso_scores, \"A0A420MWB1_Fusarium_oxysporum\", \"650M\"),\n        (iso_huge_model_scores, \"A0A420MWB1_Fusarium_oxysporum\", \"3B\"),\n    ],\n    iso_tree,\n    normalize=True,\n)\nplot_multiple_evo_lines(\n    [\n        (iso_small_model_scores, \"A0A177DQL5_Alternaria_alternata\", \"8M\"),\n        (iso_med_model_scores, \"A0A177DQL5_Alternaria_alternata\", \"35M\"),\n        (iso_large_model_scores, \"A0A177DQL5_Alternaria_alternata\", \"150M\"),\n        (iso_scores, \"A0A177DQL5_Alternaria_alternata\", \"650M\"),\n        (iso_huge_model_scores, \"A0A177DQL5_Alternaria_alternata\", \"3B\"),\n    ],\n    iso_tree,\n    normalize=True,\n)\nplot_multiple_evo_lines(\n    [\n        (iso_small_model_scores, \"A0A4T0C5S8_Aureobasidium_pullulans\", \"8M\"),\n        (iso_med_model_scores, \"A0A4T0C5S8_Aureobasidium_pullulans\", \"35M\"),\n        (iso_large_model_scores, \"A0A4T0C5S8_Aureobasidium_pullulans\", \"150M\"),\n        (iso_scores, \"A0A4T0C5S8_Aureobasidium_pullulans\", \"650M\"),\n        (iso_huge_model_scores, \"A0A4T0C5S8_Aureobasidium_pullulans\", \"3B\"),\n    ],\n    iso_tree,\n    normalize=True,\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-12-output-1.png){#fig-12-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-12-output-2.png){#fig-12-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/fig-12-output-3.png){#fig-12-3}\n:::\n\nNormalized ESM2 pseudo-perplexity vs distance to tree root for different ESM2 model sizes. For each species lineage, the pseudo-perplexity scores for each model are normalized to the score for the extant sequence for that model size.\n:::\n\n\nHere, we observe a similar phenomenon where the smallest models showed largely consistent pseudo-perplexity scores. In contrast, the largest models showed more pronounced increases in pseudo-perplexity than the oldest ancestors. This pattern was consistent with ADA1 observations but less dramatic. Nonetheless, this observation is consistent with the idea that selection of ESM2 model size is essential in interpreting the plausibility of ancestral sequences relative to native.\n\n## Relationship between ASR confidence and ESM2 pseudo-perplexity\n\nThe isomaltase results prompted us to investigate whether a general relationship exists between maximum likelihood ancestral reconstruction posterior probabilities and ESM2 pseudo-perplexity. We pooled results from both ADA1 and isomaltase phylogenies and calculated pseudo-perplexity scores for all native and ancestral sequences using the 650M-parameter model.\n\n::: {#f7e5fae8 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# pull data for all remaining ADA1 sequences\nremaining_df = pd.read_csv(esm2_dir / \"ADA1_remaining_esm2_scores_650M.csv\")\nremaining_df = remaining_df[[\"sequence_id\", \"sequence\", \"pseudo_perplexity\"]]\n\n# combine with original scores\nall_scores = pd.read_csv(esm2_dir / \"ADA1_all_esm2_scores_650M.csv\")\nall_scores = all_scores[[\"sequence_id\", \"sequence\", \"pseudo_perplexity\"]]\nfull_data = pd.concat([remaining_df, all_scores])\n\n# Add in ML posterior probabilities from ASR run\nml_probs_json = \"ASR/ADA1_ASR/outputs/posterior_probabilities_no_gaps.json\"\nml_probs_json = Path(ml_probs_json)\nwith open(ml_probs_json) as f:\n    probs_dict = json.load(f)\nfull_data[\"ML prob\"] = full_data[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict[x]]) if x in probs_dict else np.nan\n)\nfull_data[\"ML prob\"] = full_data[\"ML prob\"].fillna(1.1)\n\n# pull data for all iso native and ancestor\nremaining_df = pd.read_csv(esm2_dir / \"isomaltase_remaining_esm2_scores_650M.csv\")\nremaining_df = remaining_df[[\"sequence_id\", \"sequence\", \"pseudo_perplexity\"]]\n\n# combine with original scores\niso_scores = pd.read_csv(esm2_dir / \"isomaltase_all_esm2_scores_650M.csv\")\niso_scores = iso_scores[[\"sequence_id\", \"sequence\", \"pseudo_perplexity\"]]\nfull_data_iso = pd.concat([remaining_df, iso_scores])\n\n# Retrieve ML posterior probabilities from ASR run\nml_probs_json = \"ASR/Isomaltase_ASR/outputs/posterior_probabilities_no_gaps.json\"\nml_probs_json = Path(ml_probs_json)\nwith open(ml_probs_json) as f:\n    probs_dict_iso = json.load(f)\nfull_data_iso[\"ML prob\"] = full_data_iso[\"sequence_id\"].apply(\n    lambda x: np.mean([max(x) for x in probs_dict_iso[x]]) if x in probs_dict_iso else np.nan\n)\nfull_data_iso[\"ML prob\"] = full_data_iso[\"ML prob\"].fillna(1.1)\n\n# merge all data together\nfull_data_both = pd.concat([full_data, full_data_iso])\nfull_data_both = full_data_both.drop_duplicates()\n\n# print sizes of each\nprint(f\"Total ADA1 sequences: {len(full_data)}\")\nprint(f\"Total Isomaltase sequences: {len(full_data_iso)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal ADA1 sequences: 461\nTotal Isomaltase sequences: 571\n```\n:::\n:::\n\n\nWe can then view the relationship between ML posterior probability and ESM2 pseudo-perplexity for all these sequences.\n\n::: {#cell-fig-13 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nfrom src.analysis.data_processing import violin_plot\n\nbins = [0.7, 0.8, 0.85, 0.9, 0.95, 1.0, 1.1]\nbin_labels = [\"< 0.8\", \"0.80-0.85\", \"0.85-0.90\", \"0.90-0.95\", \"0.95-1.00\", \"Extant\"]\n\nviolin_plot(full_data_both, bins, bin_labels)\n```\n\n::: {.cell-output .cell-output-display}\n![Violin plot of ESM2 pseudo-perplexity scores for both ADA1 and isomaltase sequences, binned by ML posterior probability. Horizontal bar indicates median score in each bin and number of samples in each bin indicated above.](index_files/figure-html/fig-13-output-1.png){#fig-13}\n:::\n:::\n\n\nHere we do observe a broad pattern. We see the highest median pseudo-perplexity for extant sequences. Values decrease in the 0.95-1.0 and 0.90-0.95 posterior probability bins, level off in the 0.85-0.9 bins, and begin increasing again as probabilities drop below 0.85. \n\nTo test the significance of these differences, we performed a Kruskal–Wallis and Dunn-Bonferroni post-hoc test.\n\n::: {#bf2c3dc1 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nfrom src.analysis.data_processing import kruskal_wallis_with_significant_posthoc\n\n# Perform Kruskal-Wallis test and Dunn-Benforroni and print significant comparisons\nkruskal_wallis_with_significant_posthoc(full_data_both, bins, bin_labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKruskal-Wallis test result: H = 191.825, p = 1.591e-39\n\nSignificant Dunn post hoc comparisons (adjusted p < 0.05):\nGroup 1      Group 2      Adj. p-value\n--------------------------------------\n< 0.8        0.85-0.90    4.420e-04   \n< 0.8        0.90-0.95    5.322e-06   \n< 0.8        0.95-1.00    4.800e-04   \n0.85-0.90    Extant       4.610e-06   \n0.90-0.95    Extant       5.500e-14   \n0.95-1.00    Extant       3.694e-31   \n```\n:::\n:::\n\n\nThese analyses indicate a significant effect of ML posterior probability bin on pseudo-perplexity (p << 0.001). Post hoc tests revealed that both extant sequences and the lowest-confidence ancestral reconstructions (< 0.8) exhibit significantly higher pseudo-perplexity than the high-confidence reconstructed bins. Differences among the intermediate confidence bins themselves weren't statistically significant.\n\nWhile broader generalization will require analyzing more genes, these results support the idea that ancestral sequences tend to show lower pseudo-perplexity than extant sequences, up to a point. Once mean posterior probabilities drop below ~0.85, pseudo-perplexity increases, suggesting the model becomes less confident in these reconstructed sequences (for the 650M-parameter ESM2 model).\n\n## Discussion and conclusions\n\nOur analysis suggests that ESM2 models, particularly at larger scales, capture some aspects of evolutionary signal in a context-dependent and inconsistent manner. While it's difficult to distinguish true phylogenetic understanding from overfitting to training sequences, we observe several intriguing patterns:\n\n**Lower pseudo-perplexity for ancestral sequences**. \n\nAcross both gene families and species, ESM2 often assigns lower pseudo-perplexity scores to reconstructed ancestral sequences than to extant descendants. This pattern is especially pronounced for high-confidence reconstructions, even though these ancestral sequences weren't in the training data. This behavior has been previously observed for other protein families (@kantroo_pseudo-perplexity_2024)\n\n**Discrimination between ML and consensus ancestors**. \n\nIn cases where reconstructions are well-supported, ESM2 prefers maximum likelihood ancestors over simple consensus sequences, suggesting sensitivity to subtle features of evolutionary plausibility beyond mere amino acid frequency. However, this preference weakens for ancient or low-confidence ancestors, likely reflecting poor reconstruction quality or increasing divergence from the training set distribution.\n\n**Model size affects evolutionary patterns**. \n\nWhile larger ESM2 models (650M and 3B parameters) show more nuanced relationships between reconstruction confidence and evolutionary age, smaller models show flatter distributions, indicating reduced sensitivity. This suggests that some of this ability to interpret evolutionary relationships may emerge only in larger models.\n\nWhile these findings are limited to two gene families and a specific ASR approach, they support that ESM2's treatment of ancestral sequences isn't random and may reflect partial capture of phylogenetic information. Still, the patchiness of the observed patterns cautions against strong claims. Further investigation is needed to determine whether large protein language models like ESM2 genuinely encode evolutionary relationships, and orthogonal approaches may be needed to more deeply interrogate this idea. (@ektefaie_sequence_2025)\n\n## References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}