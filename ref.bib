
@article{orlandi_topiary_2023,
	title = {Topiary: {Pruning} the manual labor from ancestral sequence reconstruction},
	volume = {32},
	issn = {0961-8368},
	shorttitle = {Topiary},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9847077/},
	doi = {10.1002/pro.4551},
	abstract = {Ancestral sequence reconstruction (ASR) is a powerful tool to study the evolution of proteins and thus gain deep insight into the relationships among protein sequence, structure, and function. A major barrier to its broad use is the complexity of the task: it requires multiple software packages, complex file manipulations, and expert phylogenetic knowledge. Here we introduce topiary, a software pipeline that aims to overcome this barrier. To use topiary, users prepare a spreadsheet with a handful of sequences. Topiary then: (1) Infers the taxonomic scope for the ASR study and finds relevant sequences by BLAST; (2) Does taxonomically informed sequence quality control and redundancy reduction; (3) Constructs a multiple sequence alignment; (4) Generates a maximum‐likelihood gene tree; (5) Reconciles the gene tree to the species tree; (6) Reconstructs ancestral amino acid sequences; and (7) Determines branch supports. The pipeline returns annotated evolutionary trees, spreadsheets with sequences, and graphical summaries of ancestor quality. This is achieved by integrating modern phylogenetics software (Muscle5, RAxML‐NG, GeneRax, and PastML) with online databases (NCBI and the Open Tree of Life). In this paper, we introduce non‐expert readers to the steps required for ASR, describe the specific design choices made in topiary, provide a detailed protocol for users, and then validate the pipeline using datasets from a broad collection of protein families. Topiary is freely available for download: https://github.com/harmslab/topiary.},
	number = {2},
	urldate = {2025-07-21},
	journal = {Protein Science : A Publication of the Protein Society},
	author = {Orlandi, Kona N. and Phillips, Sophia R. and Sailer, Zachary R. and Harman, Joseph L. and Harms, Michael J.},
	month = feb,
	year = {2023},
	pmid = {36565302},
	pmcid = {PMC9847077},
	pages = {e4551},
	file = {Full Text:/Users/isabel/Zotero/storage/Y93EPNAU/Orlandi et al. - 2023 - Topiary Pruning the manual labor from ancestral sequence reconstruction.pdf:application/pdf},
}

@article{lin_evolutionary-scale_2023,
	title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
	volume = {379},
	url = {https://www.science.org/doi/10.1126/science.ade2574},
	doi = {10.1126/science.ade2574},
	abstract = {Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for {\textgreater}617 million metagenomic protein sequences, including {\textgreater}225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.},
	number = {6637},
	urldate = {2025-07-21},
	journal = {Science},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
	month = mar,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1123--1130},
}

@misc{kantroo_pseudo-perplexity_2024,
	title = {Pseudo-perplexity in {One} {Fell} {Swoop} for {Protein} {Fitness} {Estimation}},
	url = {http://arxiv.org/abs/2407.07265},
	doi = {10.48550/arXiv.2407.07265},
	abstract = {Protein language models trained on the masked language modeling objective learn to predict the identity of hidden amino acid residues within a sequence using the remaining observable sequence as context. They do so by embedding the residues into a high dimensional space that encapsulates the relevant contextual cues. These embedding vectors serve as an informative context-sensitive representation that not only aids with the defined training objective, but can also be used for other tasks by downstream models. We propose a scheme to use the embeddings of an unmasked sequence to estimate the corresponding masked probability vectors for all the positions in a single forward pass through the language model. This One Fell Swoop (OFS) approach allows us to efficiently estimate the pseudo-perplexity of the sequence, a measure of the model's uncertainty in its predictions, that can also serve as a fitness estimate. We find that ESM2 OFS pseudo-perplexity performs nearly as well as the true pseudo-perplexity at fitness estimation, and more notably it defines a new state of the art on the ProteinGym Indels benchmark. The strong performance of the fitness measure prompted us to investigate if it could be used to detect the elevated stability reported in reconstructed ancestral sequences. We find that this measure ranks ancestral reconstructions as more fit than extant sequences. Finally, we show that the computational efficiency of the technique allows for the use of Monte Carlo methods that can rapidly explore functional sequence space.},
	urldate = {2025-07-21},
	publisher = {arXiv},
	author = {Kantroo, Pranav and Wagner, Günter P. and Machta, Benjamin B.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07265 [q-bio]},
	keywords = {Quantitative Biology - Biomolecules, Quantitative Biology - Genomics},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/2GV4XYD5/Kantroo et al. - 2024 - Pseudo-perplexity in One Fell Swoop for Protein Fitness Estimation.pdf:application/pdf;Snapshot:/Users/isabel/Zotero/storage/M24P8QKE/2407.html:text/html},
}

@article{hochberg_reconstructing_2017,
	title = {Reconstructing {Ancient} {Proteins} to {Understand} the {Causes} of {Structure} and {Function}},
	volume = {46},
	issn = {1936-122X, 1936-1238},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-biophys-070816-033631},
	doi = {10.1146/annurev-biophys-070816-033631},
	abstract = {A central goal in biochemistry is to explain the causes of protein sequence, structure, and function. Mainstream approaches seek to rationalize sequence and structure in terms of their effects on function and to identify function’s underlying determinants by comparing related proteins to each other. Although productive, both strategies suffer from intrinsic limitations that have left important aspects of many proteins unexplained. These limits can be overcome by reconstructing ancient proteins, experimentally characterizing their properties, and retracing their evolution through time. This approach has proven to be a powerful means for discovering how historical changes in sequence produced the functions, structures, and other physical/chemical characteristics of modern proteins. It has also illuminated whether protein features evolved because of functional optimization, historical constraint, or blind chance. Here we review recent studies employing ancestral protein reconstruction and show how they have produced new knowledge not only of molecular evolutionary processes but also of the underlying determinants of modern proteins’ physical, chemical, and biological properties.},
	language = {en},
	number = {1},
	urldate = {2025-07-22},
	journal = {Annual Review of Biophysics},
	author = {Hochberg, Georg K. A. and Thornton, Joseph W.},
	month = may,
	year = {2017},
	note = {Publisher: Annual Reviews},
	pages = {247--269},
	file = {PDF:/Users/isabel/Zotero/storage/Z66D98L8/Hochberg and Thornton - 2017 - Reconstructing Ancient Proteins to Understand the Causes of Structure and Function.pdf:application/pdf},
}

@article{tule_protein_2025,
	title = {Do protein language models learn phylogeny?},
	volume = {26},
	issn = {1477-4054},
	url = {https://doi.org/10.1093/bib/bbaf047},
	doi = {10.1093/bib/bbaf047},
	abstract = {Deep machine learning demonstrates a capacity to uncover evolutionary relationships directly from protein sequences, in effect internalising notions inherent to classical phylogenetic tree inference. We connect these two paradigms by assessing the capacity of protein-based language models (pLMs) to discern phylogenetic relationships without being explicitly trained to do so. We evaluate ESM2, ProtTrans, and MSA-Transformer relative to classical phylogenetic methods, while also considering sequence insertions and deletions (indels) across 114 Pfam datasets. The largest ESM2 model tends to outperform other pLMs (including the multimodal ESM3) by recovering phylogenetic relationships among homologous protein sequences in both low- and high-gap settings. pLMs agree with conventional phylogenetic methods in general, but more so for protein families with fewer implied indels, highlighting indels as a key factor differentiating classical phylogenetics from pLMs. We find that pLMs preferentially capture broader as opposed to finer evolutionary relationships within a specific protein family, where ESM2 has a sweet spot for highly divergent sequences, at remote distance. Less than 10\% of neurons are sufficient to broadly recapitulate classical phylogenetic distances; when used in isolation, the difference between the paradigms is further diminished. We show these neurons are polysemantic, shared among different homologous families but never fully overlapping. We highlight the potential of ESM2 as a complementary tool for phylogenetic analysis, especially when extending to remote homologs that are difficult to align and imply complex histories of insertions and deletions. Implementations of analyses are available at https://github.com/santule/pLMEvo.},
	number = {1},
	urldate = {2025-07-22},
	journal = {Briefings in Bioinformatics},
	author = {Tule, Sanjana and Foley, Gabriel and Bodén, Mikael},
	month = jan,
	year = {2025},
	pages = {bbaf047},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/2D8JN3Y2/Tule et al. - 2025 - Do protein language models learn phylogeny.pdf:application/pdf;Snapshot:/Users/isabel/Zotero/storage/7R7BNFRY/bbaf047.html:text/html},
}

@article{hayes_simulating_2025,
	title = {Simulating 500 million years of evolution with a language model},
	volume = {387},
	url = {https://www.science.org/doi/10.1126/science.ads0018},
	doi = {10.1126/science.ads0018},
	abstract = {More than 3 billion years of evolution have produced an image of biology encoded into the space of natural proteins. Here, we show that language models trained at scale on evolutionary data can generate functional proteins that are far away from known proteins. We present ESM3, a frontier multimodal generative language model that reasons over the sequence, structure, and function of proteins. ESM3 can follow complex prompts combining its modalities and is highly responsive to alignment to improve its fidelity. We have prompted ESM3 to generate fluorescent proteins. Among the generations that we synthesized, we found a bright fluorescent protein at a far distance (58\% sequence identity) from known fluorescent proteins, which we estimate is equivalent to simulating 500 million years of evolution.},
	number = {6736},
	urldate = {2025-07-22},
	journal = {Science},
	author = {Hayes, Thomas and Rao, Roshan and Akin, Halil and Sofroniew, Nicholas J. and Oktay, Deniz and Lin, Zeming and Verkuil, Robert and Tran, Vincent Q. and Deaton, Jonathan and Wiggert, Marius and Badkundri, Rohil and Shafkat, Irhum and Gong, Jun and Derry, Alexander and Molina, Raul S. and Thomas, Neil and Khan, Yousuf A. and Mishra, Chetan and Kim, Carolyn and Bartie, Liam J. and Nemeth, Matthew and Hsu, Patrick D. and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
	month = feb,
	year = {2025},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {850--858},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/JBLMZPLF/Hayes et al. - 2025 - Simulating 500 million years of evolution with a language model.pdf:application/pdf},
}

@misc{bhatnagar_scaling_2025,
	title = {Scaling unlocks broader generation and deeper functional understanding of proteins},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.04.15.649055v1},
	doi = {10.1101/2025.04.15.649055},
	abstract = {Generative protein language models (PLMs) are powerful tools for designing proteins purpose-built to solve problems in medicine, agriculture, and industrial processes. Recent work has trained ever larger language models, but there has been little systematic study of the optimal training distributions and the influence of model scale on the sequences generated by PLMs. We introduce the ProGen3 family of sparse generative PLMs, and we develop compute-optimal scaling laws to scale up to a 46B-parameter model pre-trained on 1.5T amino acid tokens. ProGen3’s pre-training data is sampled from an optimized data distribution over the Profluent Protein Atlas v1, a carefully curated dataset of 3.4B full-length proteins. We evaluate for the first time in the wet lab the influence of model scale on the sequences generated by PLMs, and we find that larger models generate viable proteins for a much wider diversity of protein families. Finally, we find both computationally and experimentally that larger models are more responsive to alignment with laboratory data, resulting in improved protein fitness prediction and sequence generation capabilities. These results indicate that larger PLMs like ProGen3-46B trained on larger, well-curated datasets are powerful foundation models that push the frontier of protein design.1},
	language = {en},
	urldate = {2025-07-22},
	publisher = {bioRxiv},
	author = {Bhatnagar, Aadyot and Jain, Sarthak and Beazer, Joel and Curran, Samuel C. and Hoffnagle, Alexander M. and Ching, Kyle and Martyn, Michael and Nayfach, Stephen and Ruffolo, Jeffrey A. and Madani, Ali},
	month = apr,
	year = {2025},
	note = {Pages: 2025.04.15.649055
Section: New Results},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/IVPEIXGS/Bhatnagar et al. - 2025 - Scaling unlocks broader generation and deeper functional understanding of proteins.pdf:application/pdf},
}

@article{lupo_protein_2022,
	title = {Protein language models trained on multiple sequence alignments learn phylogenetic relationships},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-34032-y},
	doi = {10.1038/s41467-022-34032-y},
	abstract = {Self-supervised neural language models with attention have recently been applied to biological sequence data, advancing structure, function and mutational effect prediction. Some protein language models, including MSA Transformer and AlphaFold’s EvoFormer, take multiple sequence alignments (MSAs) of evolutionarily related proteins as inputs. Simple combinations of MSA Transformer’s row attentions have led to state-of-the-art unsupervised structural contact prediction. We demonstrate that similarly simple, and universal, combinations of MSA Transformer’s column attentions strongly correlate with Hamming distances between sequences in MSAs. Therefore, MSA-based language models encode detailed phylogenetic relationships. We further show that these models can separate coevolutionary signals encoding functional and structural constraints from phylogenetic correlations reflecting historical contingency. To assess this, we generate synthetic MSAs, either without or with phylogeny, from Potts models trained on natural MSAs. We find that unsupervised contact prediction is substantially more resilient to phylogenetic noise when using MSA Transformer versus inferred Potts models.},
	language = {en},
	number = {1},
	urldate = {2025-07-22},
	journal = {Nature Communications},
	author = {Lupo, Umberto and Sgarbossa, Damiano and Bitbol, Anne-Florence},
	month = oct,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Phylogeny, Protein sequence analyses},
	pages = {6298},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/9432B7IP/Lupo et al. - 2022 - Protein language models trained on multiple sequence alignments learn phylogenetic relationships.pdf:application/pdf},
}

@article{york_phylogenies_2025,
	title = {Phylogenies and biological foundation models},
	issn = {2998-4084},
	url = {https://research.arcadiascience.com/pub/idea-phylogenies-bfms/release/1/},
	doi = {10.57844/arcadia-znum-bm22},
	abstract = {Biological foundation models are, at their core, evolutionary comparisons on massive scales. As with all comparative studies, evolutionary nonindependence determines their power. We chart how this affects biological AI and propose practical routes to set the field on firmer ground.},
	language = {en},
	urldate = {2025-07-22},
	author = {York, Ryan and Bell, Audrey and Avasthi, Prachee and McGeever, Erin},
	month = may,
	year = {2025},
	note = {Publisher: Arcadia Science},
	file = {Snapshot:/Users/isabel/Zotero/storage/UWKEFJ4B/1.html:text/html},
}

@article{voordeckers_reconstruction_2012,
	title = {Reconstruction of {Ancestral} {Metabolic} {Enzymes} {Reveals} {Molecular} {Mechanisms} {Underlying} {Evolutionary} {Innovation} through {Gene} {Duplication}},
	volume = {10},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001446},
	doi = {10.1371/journal.pbio.1001446},
	abstract = {Resurrection of ancient fungal maltase enzymes uncovers the molecular details of how repeated gene duplications allow the evolution of protein variants with different functions.},
	language = {en},
	number = {12},
	urldate = {2025-07-22},
	journal = {PLOS Biology},
	author = {Voordeckers, Karin and Brown, Chris A. and Vanneste, Kevin and Zande, Elisa van der and Voet, Arnout and Maere, Steven and Verstrepen, Kevin J.},
	month = dec,
	year = {2012},
	note = {Publisher: Public Library of Science},
	keywords = {Enzymes, Evolutionary genetics, Fungal evolution, Molecular evolution, Phylogenetic analysis, Phylogenetics, Saccharomyces cerevisiae, Yeast},
	pages = {e1001446},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/QDP2SM9A/Voordeckers et al. - 2012 - Reconstruction of Ancestral Metabolic Enzymes Reveals Molecular Mechanisms Underlying Evolutionary I.pdf:application/pdf},
}

@article{bridgham2006,
  title = {Evolution of Hormone-Receptor Complexity by Molecular Exploitation},
  author = {Bridgham, Jamie T. and Carroll, Sean M. and Thornton, Joseph W.},
  journal = {Science},
  volume = {312},
  number = {5770},
  pages = {97--101},
  year = {2006},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1123348}
}

@article{wilson_using_2015,
	title = {Using ancient protein kinases to unravel a modern cancer drug’s mechanism},
	volume = {347},
	url = {https://www.science.org/doi/abs/10.1126/science.aaa1823},
	doi = {10.1126/science.aaa1823},
	abstract = {Macromolecular function is rooted in energy landscapes, where sequence determines not a single structure but an ensemble of conformations. Hence, evolution modifies a protein’s function by altering its energy landscape. Here, we recreate the evolutionary pathway between two modern human oncogenes, Src and Abl, by reconstructing their common ancestors. Our evolutionary reconstruction combined with x-ray structures of the common ancestor and pre–steady-state kinetics reveals a detailed atomistic mechanism for selectivity of the successful cancer drug Gleevec. Gleevec affinity is gained during the evolutionary trajectory toward Abl and lost toward Src, primarily by shifting an induced-fit equilibrium that is also disrupted in the clinical T315I resistance mutation. This work reveals the mechanism of Gleevec specificity while offering insights into how energy landscapes evolve.},
	number = {6224},
	urldate = {2025-07-22},
	journal = {Science},
	author = {Wilson, C. and Agafonov, R. V. and Hoemberger, M. and Kutter, S. and Zorba, A. and Halpin, J. and Buosi, V. and Otten, R. and Waterman, D. and Theobald, D. L. and Kern, D.},
	month = feb,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {882--886},
	file = {Accepted Version:/Users/isabel/Zotero/storage/UC4QSGEL/Wilson et al. - 2015 - Using ancient protein kinases to unravel a modern cancer drug’s mechanism.pdf:application/pdf},
}

@inproceedings{salazar_masked_2020,
	address = {Online},
	title = {Masked {Language} {Model} {Scoring}},
	url = {https://aclanthology.org/2020.acl-main.240/},
	doi = {10.18653/v1/2020.acl-main.240},
	abstract = {Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30\% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.},
	urldate = {2025-07-22},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {2699--2712},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/7SFK8JF5/Salazar et al. - 2020 - Masked Language Model Scoring.pdf:application/pdf},
}

@misc{ektefaie_sequence_2025,
	title = {Sequence {Modeling} {Is} {Not} {Evolutionary} {Reasoning}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.01.17.633626v2},
	doi = {10.1101/2025.01.17.633626},
	abstract = {Protein language models (PLMs) are commonly assumed to capture evolutionary information by training on large protein sequence datasets. However, it remains unclear whether PLMs can reason about evolution—that is, infer evolutionary relationships between protein sequences. To test this capability, we introduce a benchmark for evolutionary reasoning and find that existing PLMs consistently fail to recover phylogenetic structure, despite strong performance on standard tasks such as masked token prediction and contact prediction. To address this limitation, we present Phyla. Phylaintroduces a hybrid state-space and transformer architecture that jointly process multiple sequences and is trained using a tree-based objective over 3,000 phylogenies spanning diverse protein families. Phylaachieves state-of-the-art performance in evolutionary reasoning, outper-forming the next-best model by 13\% on tree reconstruction and 10\% on taxonomic clustering. Beyond synthetic benchmarks, Phylaapplies to real-world settings: it reconstructs biologically accurate branches of the tree of life and infers whole-genome evolutionary relationships from Mycobacterium tuberculosis genomes. These findings suggest that evolutionary reasoning is not an emergent property of large-scale sequence modeling. Instead, Phylashows that models trained with phylogenetic supervision can reason about evolution more effectively, offering a biologically grounded path toward evolutionary foundation models.},
	language = {en},
	urldate = {2025-07-22},
	publisher = {bioRxiv},
	author = {Ektefaie, Yasha and Shen, Andrew and Jain, Lavik and Farhat, Maha and Zitnik, Marinka},
	month = jun,
	year = {2025},
	note = {Pages: 2025.01.17.633626
Section: New Results},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/B2PRVD96/Ektefaie et al. - 2025 - Sequence Modeling Is Not Evolutionary Reasoning.pdf:application/pdf},
}


@article{avasthi_proteincartography_2023,
	title = {{ProteinCartography}: {Comparing} proteins with structure-based maps for interactive exploration},
	issn = {2998-4084},
	shorttitle = {{ProteinCartography}},
	url = {https://research.arcadiascience.com/pub/resource-protein-cartography/release/11/},
	doi = {10.57844/arcadia-a5a6-1068},
	abstract = {The ProteinCartography pipeline identifies proteins related to a query protein using sequence- and structure-based searches, compares all protein structures, and creates a navigable map that can be used to look at protein relationships and make hypotheses about function.},
	language = {en},
	urldate = {2025-07-28},
	author = {Avasthi, Prachee and Bigge, Brae M. and Borges, Adair L. and Celebi, Feridun Mert and Cheveralls, Keith and Dutton, Rachel J. and Gehring, Jase and Hochstrasser, Megan L. and Iii, Galo Garcia and MacQuarrie, Cameron Dale and Matus, David Q. and McDaniel, Elizabeth A. and McGeever, Erin and Mishne, Gilad and Morin, Manon and Radkov, Atanas and Reiter, Taylor and Reitman, Michael E. and Sun, Dennis A. and Weiss, Emily C. P.},
	month = aug,
	year = {2023},
	note = {Publisher: Arcadia Science},
	file = {Snapshot:/Users/isabel/Zotero/storage/GW6UNDL6/11.html:text/html},
}



@article{nguyen_iq-tree_2015,
	title = {{IQ}-{TREE}: {A} {Fast} and {Effective} {Stochastic} {Algorithm} for {Estimating} {Maximum}-{Likelihood} {Phylogenies}},
	volume = {32},
	issn = {0737-4038},
	shorttitle = {{IQ}-{TREE}},
	url = {https://doi.org/10.1093/molbev/msu300},
	doi = {10.1093/molbev/msu300},
	abstract = {Large phylogenomics data sets require fast tree inference methods, especially for maximum-likelihood (ML) phylogenies. Fast programs exist, but due to inherent heuristics to find optimal trees, it is not clear whether the best tree is found. Thus, there is need for additional approaches that employ different search strategies to find ML trees and that are at the same time as fast as currently available ML programs. We show that a combination of hill-climbing approaches and a stochastic perturbation method can be time-efficiently implemented. If we allow the same CPU time as RAxML and PhyML, then our software IQ-TREE found higher likelihoods between 62.2\% and 87.1\% of the studied alignments, thus efficiently exploring the tree-space. If we use the IQ-TREE stopping rule, RAxML and PhyML are faster in 75.7\% and 47.1\% of the DNA alignments and 42.2\% and 100\% of the protein alignments, respectively. However, the range of obtaining higher likelihoods with IQ-TREE improves to 73.3–97.1\%. IQ-TREE is freely available at http://www.cibiv.at/software/iqtree.},
	number = {1},
	urldate = {2025-07-28},
	journal = {Molecular Biology and Evolution},
	author = {Nguyen, Lam-Tung and Schmidt, Heiko A. and von Haeseler, Arndt and Minh, Bui Quang},
	month = jan,
	year = {2015},
	pages = {268--274},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/3J3MXIKH/Nguyen et al. - 2015 - IQ-TREE A Fast and Effective Stochastic Algorithm for Estimating Maximum-Likelihood Phylogenies.pdf:application/pdf;Snapshot:/Users/isabel/Zotero/storage/5XA87IH3/msu300.html:text/html},
}

@article{yang_paml_2007,
	title = {{PAML} 4: {Phylogenetic} {Analysis} by {Maximum} {Likelihood}},
	volume = {24},
	issn = {0737-4038},
	shorttitle = {{PAML} 4},
	url = {https://doi.org/10.1093/molbev/msm088},
	doi = {10.1093/molbev/msm088},
	abstract = {PAML, currently in version 4, is a package of programs for phylogenetic analyses of DNA and protein sequences using maximum likelihood (ML). The programs may be used to compare and test phylogenetic trees, but their main strengths lie in the rich repertoire of evolutionary models implemented, which can be used to estimate parameters in models of sequence evolution and to test interesting biological hypotheses. Uses of the programs include estimation of synonymous and nonsynonymous rates (dN and dS) between two protein-coding DNA sequences, inference of positive Darwinian selection through phylogenetic comparison of protein-coding genes, reconstruction of ancestral genes and proteins for molecular restoration studies of extinct life forms, combined analysis of heterogeneous data sets from multiple gene loci, and estimation of species divergence times incorporating uncertainties in fossil calibrations. This note discusses some of the major applications of the package, which includes example data sets to demonstrate their use. The package is written in ANSI C, and runs under Windows, Mac OSX, and UNIX systems. It is available at http://abacus.gene.ucl.ac.uk/software/paml.html.},
	number = {8},
	urldate = {2025-07-28},
	journal = {Molecular Biology and Evolution},
	author = {Yang, Ziheng},
	month = aug,
	year = {2007},
	pages = {1586--1591},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/X4CIHM65/Yang - 2007 - PAML 4 Phylogenetic Analysis by Maximum Likelihood.pdf:application/pdf;Snapshot:/Users/isabel/Zotero/storage/N382SZM5/msm088.html:text/html},
}

@article{katoh_mafft_2002,
	title = {{MAFFT}: a novel method for rapid multiple sequence alignment based on fast {Fourier} transform},
	volume = {30},
	issn = {0305-1048},
	shorttitle = {{MAFFT}},
	url = {https://doi.org/10.1093/nar/gkf436},
	doi = {10.1093/nar/gkf436},
	abstract = {A multiple sequence alignment program, MAFFT, has been developed. The CPU time is drastically reduced as compared with existing methods. MAFFT includes two novel techniques. (i) Homo logous regions are rapidly identified by the fast Fourier transform (FFT), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) We propose a simplified scoring system that performs well for reducing CPU time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. Two different heuristics, the progressive method (FFT‐NS‐2) and the iterative refinement method (FFT‐NS‐i), are implemented in MAFFT. The performances of FFT‐NS‐2 and FFT‐NS‐i were compared with other methods by computer simulations and benchmark tests; the CPU time of FFT‐NS‐2 is drastically reduced as compared with CLUSTALW with comparable accuracy. FFT‐NS‐i is over 100 times faster than T‐COFFEE, when the number of input sequences exceeds 60, without sacrificing the accuracy.},
	number = {14},
	urldate = {2025-07-28},
	journal = {Nucleic Acids Research},
	author = {Katoh, Kazutaka and Misawa, Kazuharu and Kuma, Kei‐ichi and Miyata, Takashi},
	month = jul,
	year = {2002},
	pages = {3059--3066},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/AQTGHHGZ/Katoh et al. - 2002 - MAFFT a novel method for rapid multiple sequence alignment based on fast Fourier transform.pdf:application/pdf;Snapshot:/Users/isabel/Zotero/storage/RVB3M5U4/gkf436.html:text/html},
}

@article{ishikawa_fast_2019,
	title = {A {Fast} {Likelihood} {Method} to {Reconstruct} and {Visualize} {Ancestral} {Scenarios}},
	volume = {36},
	issn = {0737-4038},
	url = {https://doi.org/10.1093/molbev/msz131},
	doi = {10.1093/molbev/msz131},
	abstract = {The reconstruction of ancestral scenarios is widely used to study the evolution of characters along phylogenetic trees. One commonly uses the marginal posterior probabilities of the character states, or the joint reconstruction of the most likely scenario. However, marginal reconstructions provide users with state probabilities, which are difficult to interpret and visualize, whereas joint reconstructions select a unique state for every tree node and thus do not reflect the uncertainty of inferences.We propose a simple and fast approach, which is in between these two extremes. We use decision-theory concepts (namely, the Brier score) to associate each node in the tree to a set of likely states. A unique state is predicted in tree regions with low uncertainty, whereas several states are predicted in uncertain regions, typically around the tree root. To visualize the results, we cluster the neighboring nodes associated with the same states and use graph visualization tools. The method is implemented in the PastML program and web server.The results on simulated data demonstrate the accuracy and robustness of the approach. PastML was applied to the phylogeography of Dengue serotype 2 (DENV2), and the evolution of drug resistances in a large HIV data set. These analyses took a few minutes and provided convincing results. PastML retrieved the main transmission routes of human DENV2 and showed the uncertainty of the human-sylvatic DENV2 geographic origin. With HIV, the results show that resistance mutations mostly emerge independently under treatment pressure, but resistance clusters are found, corresponding to transmissions among untreated patients.},
	number = {9},
	urldate = {2025-07-28},
	journal = {Molecular Biology and Evolution},
	author = {Ishikawa, Sohta A and Zhukova, Anna and Iwasaki, Wataru and Gascuel, Olivier},
	month = sep,
	year = {2019},
	pages = {2069--2085},
	file = {Full Text PDF:/Users/isabel/Zotero/storage/FCPJS2G9/Ishikawa et al. - 2019 - A Fast Likelihood Method to Reconstruct and Visualize Ancestral Scenarios.pdf:application/pdf;Snapshot:/Users/isabel/Zotero/storage/CANRMYWW/msz131.html:text/html},
}